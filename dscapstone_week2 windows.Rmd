---
title: "Data Science Capstone - Week 2 Milestone Report"
author: "Linda Zahora-Cathcart"
date: Nov 7, 2019
output: html_document
---
### OVERVIEW
Whenever you're designing a new product it's important to establish certain goals from the beginning to ensure that you are either on track or staying the course. For instance, with this particular Week 2 Milestone Report for our Data Science Project our goal is to demonstrate that with the data we have been given we are creating and planning our prediction algorithm correctly. We are expected to do the following:

1. Submit a report on R Pubs that explains your exploratory analysis and your goals for the eventual app and algorithm. This document should be concise and explain only the major features of the data you have identified and briefly summarize your plans for creating the prediction algorithm and Shiny app in a way that would be understandable to a non-data scientist manager. 

2. You should make use of tables and plots to illustrate important summaries of the data set. The motivation for this project is to: 
* Demonstrate that you've downloaded the data and have successfully loaded it in.
* Create a basic report of summary statistics about the data sets.
* Report any interesting findings that you amassed so far.
* Get feedback on your plans for creating a prediction algorithm and Shiny app.

### LOAD LIBRARIES
```{r, message = FALSE, warning = FALSE}
library(quanteda)
library(ngram)
library(ggplot2)
library(reshape2)
library(RColorBrewer)
library(cowplot)
```

### LOAD and READ DATA
Initially loading in the data was challenging, but a file connection in binary mode solved the issue which was created from the control Z character.

```{r}
blogsfile <- "final/en_US/en_US.blogs.txt"
con <- file(blogsfile, open = "r"); 
blogs <- readLines(con, encoding = "UTF-8", skipNul = TRUE); close(con)

newsfile <- "final/en_US/en_US.news.txt"
con <- file(newsfile, open = "r"); 
news <- readLines(con, encoding = "UTF-8", skipNul = TRUE); close(con)

twitterfile <- "final/en_US/en_US.twitter.txt"
con <- file(twitterfile, open = "r"); 
twitter <- readLines(con, encoding = "UTF-8", skipNul = TRUE); close(con)
```

### BASIC ANALYSIS and FIRST STEPS
After loading and reading in our dataset a basic analysis will be done to get better acquainted with our data. We will be looking for the following statistics for each of the three text corporas: size, count of words, count of lines as well as the amount of characters. Then, we'll plot a few of our answers for a visual representation.

Calculate the size of each corpora.
```{r}
object.size(blogs)
object.size(news)
object.size(twitter)
```

Calculate the amount of lines in the data.
```{r}
length(blogs)
length(news)
length(twitter)
```

Calculate the word count in the data and create a histogram.
```{r, fig.align='center'}
bcount <- wordcount(blogs, sep = " ")
ncount <- wordcount(news,  sep = " ")
tcount <- wordcount(twitter, sep = " ")

# cbind corporas into a dataframe      
totalwordcount <- cbind(bcount, ncount, tcount)
        colnames(totalwordcount) <- c("Blog", "News", "Twitter")
        totalwordcount <- melt(totalwordcount)
        colnames(totalwordcount) <- c("Var1", "Source", "WordCount")
       
        wordcountplot <- ggplot(totalwordcount, aes(x=Source, y=WordCount,  
                         fill=WordCount)) +
                         geom_bar(stat='identity') + 
                         coord_flip() +
                         scale_colour_hue(h = c(0, 90)) +
                         xlab("Word Count by each Source") + 
                         ylab("Word Count")
       
        wordcountplot
```

Calculate the distribution of characters per line and show with a boxplot.
```{r, fig.align='center'}
blogsCHAR <- nchar(blogs)
newsCHAR  <- nchar(news)
twitterCHAR <- nchar(twitter)

boxplot(blogsCHAR, newsCHAR, twitterCHAR, log = "y",
        names = c("blogs", "news", "twitter"),
        ylab = "Characters per Line", xlab = "Source") 
title("Distributions of Characters per Line")

```

### SAMPLING
Due to the large datasets we're using it will be difficult processing many of the functions that would be needed to do exploratory analysis and further. With that being said, after reading an article by Len Greski on how different computer models (as well as their RAM capabilities) handle large datasets we're going to take approximately a 25% sample size from each of the text files to assist with performance timings in the future.

```{r, warning=FALSE}
set.seed(1011)
size <- 355807

blogs <- blogs[rbinom(size, length(blogs), 0.5)]
news <- news[rbinom(size, length(news), 0.5)]
twitter <- twitter[rbinom(size, length(twitter), 0.5)]
```

### CORPUS CREATION
Now that we have a good idea what our data looks like and we have our sample size it's time to move forward to our next step and create our corpus. We'll need to do the following in this step:

* Create individual corpus files for each data source
* Combine our three corpus files into one
* Preprocess our corpus by creating a token object and removing unneeded elements
* Tokenize our preprocessed corpus into sentences and words
* And then apply a profanity filter

```{r}
# create individual corpus files.
corpusBLOGS <- corpus(blogs)
docvars(corpusBLOGS, 'Source') <- 'blogs'

corpusNEWS <- corpus(news)
docvars(corpusNEWS, 'Source') <- 'news'

corpusTWITTER <- corpus(twitter)
docvars(corpusTWITTER, 'Source') <- 'twitter'
```

Combine corpus files into one.
```{r}
allCORPUS <- corpusBLOGS + corpusNEWS + corpusTWITTER
summaryCORPUS <- summary(allCORPUS)
```

### PRE-PROCESSING CORPUS
Construct a tokens object from your combined corpus. 
```{r}
#create tokens object by master list
tokensCORPUS <-tokens(allCORPUS, 
                remove_numbers = T, remove_punct = T,
                remove_symbols = T, remove_separators = T,
                remove_twitter = T, remove_hyphens = T,
                remove_url = T)

#create tokens object by source
tokensBLOG <- tokens(corpusBLOGS, 
                remove_numbers = T, remove_punct = T,
                remove_symbols = T, remove_separators = T,
                remove_twitter = T, remove_hyphens = T,
                remove_url = T)

tokensNEWS <- tokens(corpusNEWS, 
                remove_numbers = T, remove_punct = T,
                remove_symbols = T, remove_separators = T,
                remove_twitter = T, remove_hyphens = T,
                remove_url = T)

tokensTWITTER <- tokens(corpusTWITTER, 
                remove_numbers = T, remove_punct = T,
                remove_symbols = T, remove_separators = T,
                remove_twitter = T, remove_hyphens = T,
                remove_url = T)
```

### TOKENIZATION
Tokenize corpus into sentences.
```{r}
tokensSENTENCE <- tokens(tokensCORPUS, what = "sentence")
```

Tokenize corpus into words.
```{r}
#tokenize into words by master list
tokensWORDS <- tokens(tokensCORPUS, what = "word")

#tokenize into words by source
blogWORDS <- tokens(tokensBLOG, what = "word")
newsWORDS <- tokens(tokensNEWS, what = "word")
twitterWORDS <- tokens(tokensTWITTER, what = "word")
```

### PROFANITY FILTERING
```{r}
profanity <- readLines('profanityfilter.txt', skipNul = T)

#profanity removal using master list  
removeprofanity <- tokens_remove(tokensWORDS, pattern = profanity)

#profanity removal by source
blogWORDS <- tokens_remove(blogWORDS, pattern = profanity)
newsWORDS <- tokens_remove(newsWORDS, pattern = profanity)
twitterWORDS <- tokens_remove(twitterWORDS, pattern = profanity)
```

### CREATE A DOCUMENT FEATURE MATRIX (DFM) OBJECT
The next step we will be doing is creating a DFM object. This step will allow us to further process and analyze our data. For instance, we'll be looking at the significance that our stopwords do or do not have on word frequencies and then we'll look at other visualizations. 

```{r}
# with stopwords
dfmSTOPS <- dfm(tokensWORDS,tolower=TRUE)
dfmST_stat <- textstat_frequency(dfmSTOPS)

# without stopwords
dfmNOSTOPS <- dfm(tokensWORDS,tolower=TRUE, remove=stopwords("english"))
dfmNOST_stat <- textstat_frequency(dfmNOSTOPS)

#create dfm by source
blogDFM <- dfm(blogWORDS,tolower=TRUE, remove=stopwords("english"))
blogDFM_stats <- textstat_frequency(blogDFM) 

newsDFM <- dfm(newsWORDS,tolower=TRUE, remove=stopwords("english"))
newsDFM_stats <- textstat_frequency(newsDFM) 

twitterDFM <- dfm(twitterWORDS,tolower=TRUE, remove=stopwords("english"))
twitterDFM_stats <- textstat_frequency(twitterDFM) 
```

### REMOVE OLD DATA
Remove old data to free up memory space.
```{r}
rm(allCORPUS, blogDFM, blogWORDS, corpusBLOGS, corpusNEWS, corpusTWITTER, newsDFM, newsWORDS, removeprofanity, summaryCORPUS, tokensBLOG, tokensCORPUS, tokensNEWS, tokensSENTENCE, tokensTWITTER, totalwordcount, twitterDFM, twitterWORDS, wordcountplot, bcount, blogs, blogsCHAR, con, ncount, news, newsCHAR, profanity, size, tcount, twitter,twitterCHAR)
```

### N-GRAMS and WORD FREQUENCIES
In this next step we'll be creating most frequent words using wordclouds and bar charts for 1gram, 2gram and 3grams. 

1gram.
```{r, fig.align='center'}
# wordcloud created for unigram/1gram with stopwords and a max. of 200 words.
dfmSTOPS %>% textplot_wordcloud(max_words=200)

# wordcloud created for unigram/1gram without stopwords and a max. of 200 words.
dfmNOSTOPS %>% textplot_wordcloud(max_words=200)

# top 10 words with stopwords and plot creation
top10STOPS <- dfmST_stat[c(1:10),]
top10plotSTOPS <- ggplot(top10STOPS, aes(x=reorder(feature,rank) , y=frequency, label=frequency, fill=feature)) +
         geom_bar(stat='identity') + 
         scale_colour_hue(h = c(0, 90)) +
         coord_flip() +
         xlab("Top 10 Unigrams with Stopwords") + 
         ylab("Frequency")
top10plotSTOPS

# top 10 words without stopwords and plot creation
top10NOSTOPS <- dfmNOST_stat[c(1:10),] 
top10plotNOSTOPS  <- ggplot(top10NOSTOPS, aes(x=reorder(feature, rank), y=frequency, label=frequency, fill=feature)) +
         geom_bar(stat='identity') + 
         scale_colour_hue(h = c(0, 90)) +
         coord_flip() +
         xlab("Top 10 Unigrams with No Stopwords") + 
         ylab("Frequency")
top10plotNOSTOPS
```

1gram by Source.
```{r, fig.align='center'}
#three side-by-side plots of each source.
top5blog <- blogDFM_stats[c(1:5),] 
blogplot  <- ggplot(top5blog, aes(x=reorder(feature, rank), y=frequency, label=frequency, fill=feature)) +
         geom_bar(stat='identity') + 
         scale_colour_hue(h = c(0, 90)) +
         coord_flip() +
         xlab("Top 5 by Blog Source") + 
         ylab("Frequency")

top5news <- newsDFM_stats[c(1:5),] 
newsplot  <- ggplot(top5news, aes(x=reorder(feature, rank), y=frequency, label=frequency, fill=feature)) +
         geom_bar(stat='identity') + 
         coord_flip() +
         xlab("Top 5 by News Source") + 
         ylab("Frequency")

top5twitter <- twitterDFM_stats[c(1:5),] 
twitterplot  <- ggplot(top5twitter, aes(x=reorder(feature, rank), y=frequency, label=frequency, fill=feature)) +
          geom_bar(stat='identity') + 
          scale_colour_hue(h = c(0, 90)) +
          coord_flip() +
          xlab("Top 5 by Twitter Source") + 
          ylab("Frequency")

sourceplot <- plot_grid(blogplot, newsplot, twitterplot, labels = "AUTO")
sourceplot
```

Create a 2gram.
```{r, fig.align='center'}
# create 2gram 
bigram <- tokens_ngrams(tokensWORDS, n = 2L, concatenator = ' ')

# convert 2gram into a DFM object with stopwords
dfmBIGRAM <- dfm(bigram, tolower = T)
dfmBIGRAM_stat <- textstat_frequency(dfmBIGRAM)

# wordcloud created for bigram/2gram and a max. of 200 words.
dfmBIGRAM %>% textplot_wordcloud(max_words=200)

# top 10 words with stopwords and plot creation
BItop10 <- dfmBIGRAM_stat[c(1:10),]
BItop10plot <- ggplot(BItop10, aes(x=reorder(feature,rank) , y=frequency, label=frequency, fill=feature)) +
         geom_bar(stat='identity') + 
         scale_colour_hue(h = c(0, 90)) +
         coord_flip() +
         xlab("Top 10 Bigrams with Stopwords") + 
         ylab("Frequency")
BItop10plot
```

Create a 3gram.
```{r, fig.align='center'}
# create 3gram 
trigram <- tokens_ngrams(tokensWORDS, n = 3L, concatenator = ' ')

# convert 3gram into a DFM object with stopwords
dfmTRIGRAM <- dfm(trigram, tolower = T)
dfmTRIGRAM_stat <- textstat_frequency(dfmTRIGRAM)

# wordcloud created for trigram/2gram and a max. of 100 words.
dfmTRIGRAM %>% textplot_wordcloud(max_words=100)

# top 10 words with stopwords and plot creation
TRItop10 <- dfmTRIGRAM_stat[c(1:10),]
TRItop10plot <- ggplot(TRItop10, aes(x=reorder(feature,rank) , y=frequency, label=frequency, fill=feature)) +
         geom_bar(stat='identity') + 
         scale_colour_hue(h = c(0, 90)) +
         coord_flip() +
         xlab("Top 10 Trigrams with Stopwords") + 
         ylab("Frequency")
TRItop10plot
```

### CONCLUSION and NEXT STEPS
The goal for our final project for the Data Science Capstone is to be able to build a predictive algorithm that will take the form of a Shiny app. This Shiny app will need to allow an user to input a phrase (multiple words) in a text box and then predict the next word. Furthermore, the predictive algorithm will be constructed based on what was discovered while accomplishing this milestone report. 
For instance, we found that there was a significant difference when comparing unigrams with and without stopwords as well as which phrases ranked higher when comparing bigrams and trigrams.

With that being said, the next steps for continuing the final project are to:

1. Build a predictive model based on the previous data modeling steps - you may combine the models in any way you think is appropriate.
2. Evaluate the model for efficiency and accuracy - use timing software to evaluate the computational complexity of your model. Evaluate the model accuracy using different metrics like perplexity, accuracy at the first word, second word, and third word.
3. Explore new models and data to improve your predictive model.
4. Evaluate your new predictions on both accuracy and efficiency. 
5. Create a data product using your prediction algorithm.
 
### APPENDIX
The full source code and plots can be found at the below link:
https://github.com/lzcathcart/DSCapstone

### RESOURCES

Stack Overflow Reference for Control Z Character Issue:
https://stackoverflow.com/questions/15874619/reading-in-a-text-file-with-a-sub-1a-control-z-character-in-r-on-windows

Stack Overflow Reference for Processing Large Datasets on 8 GB of RAM.
https://stackoverflow.com/questions/44171534/splitting-tokenize-a-corpus-with-r-and-quanteda

N-Grams Natural Modeling:
https://lagunita.stanford.edu/c4x/Engineering/CS-224N/asset/slp4.pdf

Data Science Capstone Tips by Len Greski:
https://github.com/lgreski/datasciencectacontent/blob/master/markdown/capstone-simplifiedApproach.md

Quanteda Quick Start Guide:
https://quanteda.io/articles/quickstart.html

Google Profanity Words:
https://github.com/RobertJGabriel/Google-profanity-words/blob/master/list.txt
